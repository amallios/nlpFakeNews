{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb54b132",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "##### This coursework requires you to develop a text classifier and apply it to a specific problem or challenge, e.g. fake news detection, sentiment analysis, spam detection, document tagging, etc. You will need to identify a suitable problem area with an associated data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f409a",
   "metadata": {},
   "source": [
    "## 1. Domain-specific area\n",
    "##### The first step of the coursework is to identify and describe the problem or challenge. This is an area of industry or science where text classification methods can contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773ed82",
   "metadata": {},
   "source": [
    "The fake news detection problem refers to the identification and flagging of false or misleading information presented as news. This problem is significant because fake news can spread quickly and have serious consequences, including public confusion, a decline in trust in legitimate news sources, and even violence. One approach to addressing this problem is the use of classification methods, which involve training a model to assign inputs to predefined categories or labels. In the context of fake news detection, classification methods could be used to train a model to classify news articles as real or fake based on features such as language, source, or content. A variety of classification methods, including logistic regression, support vector machines, and decision trees, could be utilized for this task. However, accurately detecting fake news can be a complex task that may require multiple approaches and continuous model updates as new forms of fake news emerge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb7498",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "##### State and justify the objectives of the project. Discuss its impact and contribution to the problem area. State any contribution which the results may make to the challenge addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add95d9f",
   "metadata": {},
   "source": [
    "The main objective of a fake news detection project would be to develop a reliable and effective method for identifying false or misleading information presented as news, in order to mitigate the negative consequences of fake news. This could involve training a classification model to accurately distinguish between real and fake news articles, or developing a tool for detecting fake news in real-time as it is shared online.\n",
    "\n",
    "The impact and contribution of such a project would be significant, as fake news has the potential to cause widespread confusion, harm public trust in legitimate sources of information, and even incite violence. By identifying and flagging fake news, the project could help to reduce the spread of false information and protect the public from its negative effects.\n",
    "\n",
    "The results of the project could also make a significant contribution to the challenge of detecting fake news, as they could provide a new or improved method for identifying and combating this issue. This could involve the development of a new classification algorithm that outperforms existing methods, or the identification of new features or approaches that can be used to more accurately detect fake news. Additionally, the results of the project could help to inform the development of policies and strategies for addressing the problem of fake news on a larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655f8b7",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "##### The next step is to identify a suitable dataset which is representative of the challenge and will require attention to all the steps outlined in this assignment. Provide a description of the dataset, its size, data types, the way the data were acquired. State clearly the source of the dataset. Large technology companies, such as Microsoft, Google and Amazon, provide wide variety of datasets. Example: ‘Fake and real news’ dataset available from the Kaggle official website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2091179",
   "metadata": {},
   "source": [
    "We will be using the \"Fake and Real News\" dataset available from the Kaggle official website. And we can find the below information in the Provenance file of the dataset on Kaggle.\n",
    "\n",
    "##### Description of the dataset:\n",
    "The ISOT Fake News Dataset contains articles from both fake and real news sources. The real news articles were obtained by crawling articles from Reuters.com, while the fake news articles were collected from unreliable websites that were flagged by Politifact and Wikipedia. The articles cover a variety of topics, with a focus on political and world news. Each article includes the title, text, type, and date of publication. The dataset consists of two CSV files, one containing real news articles and the other containing fake news articles.\n",
    "\n",
    "##### Size of the dataset:\n",
    "The dataset consists of 21417 real news articles and 23481 fake news articles, for a total of over 44000 articles.\n",
    "\n",
    "##### Data types:\n",
    "The data in the dataset includes text and categorical variables such as article title, text, type, and date of publication.\n",
    "\n",
    "##### Method of data acquisition:\n",
    "The real news articles were obtained by crawling articles from Reuters.com, while the fake news articles were collected from unreliable websites that were flagged by Politifact and Wikipedia. The data were collected from 2016 to 2017 and were cleaned and processed, though the punctuations and mistakes in the fake news articles were retained in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae68b66",
   "metadata": {},
   "source": [
    "## 4. Evaluation methodology\n",
    "##### It is good practice in scientific research to decide in advance how you will evaluate the outputs of your investigations. Identify the evaluation metrics you will apply and how they will be applied (e.g. precision, recall, accuracy, F-measure, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed597",
   "metadata": {},
   "source": [
    "There are several evaluation metrics that could be applied to this dataset, depending on the specific goals of the analysis. Some common evaluation metrics for text classification tasks, such as identifying fake news articles, include:\n",
    "\n",
    "- Precision: Precision measures the proportion of true positive predictions among all positive predictions. For example, if the model predicts that 100 articles are fake and 90 of them are actually fake, the precision would be 90%.\n",
    "- Recall: Recall measures the proportion of true positive predictions among all actual positive cases. Continuing with the previous example, if there are 1000 total fake articles and the model correctly predicts 90 of them, the recall would be 9%.\n",
    "- Accuracy: Accuracy measures the overall proportion of correct predictions made by the model. If the model correctly predicts 900 out of 1000 total articles, the accuracy would be 90%.\n",
    "- F-measure: The F-measure combines precision and recall into a single metric by taking the harmonic mean of the two. It is often used as a summary metric for text classification tasks.\n",
    "\n",
    "We will be using F-measure to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69f8e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f641b",
   "metadata": {},
   "source": [
    "# II. Implementation\n",
    "##### This part of the coursework is the implementation of the project. It includes and preprocessing the data, building and testing your classifier and obtaining results. The project is expected to be developed using the Python language and Jupyter notebook. Provide well-commented Python code accompanied by document describing the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0adfc79",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "##### Convert/store the dataset locally and preprocess the data. Describe the text representation (e.g., bag of words, word embedding, etc.) and any pre-processing steps you have applied and why they were needed (e.g. tokenization, lemmatization). Describe the vocabulary and file type/format, e.g. CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22ed2f",
   "metadata": {},
   "source": [
    "We have saved the data locally at the location /dataset. We will now start with the process of preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e51c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18687209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load true and fake news datasets from csv files\n",
    "df_true = pd.read_csv(\"dataset/True.csv\")\n",
    "df_fake = pd.read_csv(\"dataset/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d71257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a column 'True/Fake' into each dataset indicating their respective label\n",
    "df_true.insert(1, 'True/Fake', 'true')\n",
    "df_fake.insert(1, 'True/Fake', 'fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96dd12e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_1500\\290626084.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df_true.append(df_fake)\n"
     ]
    }
   ],
   "source": [
    "# combine both datasets into one\n",
    "df=df_true.append(df_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8e3205",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake    23481\n",
       "true    21417\n",
       "Name: True/Fake, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of true and fake news in the combined dataset\n",
    "df['True/Fake'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d1acc",
   "metadata": {},
   "source": [
    "We now have a single dataframe containing the csv files we downloaded and we will now continue with the next steps of preprocessing. We have chosen to:\n",
    "\n",
    "1. Lower case all text: Converting all text to lower case helps to standardise the text and reduce the size of the vocabulary. This is because words that are capitalised may be considered as a different word compared to the lower case version of the same word.\n",
    "2. Remove punctuations and special characters: Removing punctuations and special characters helps to reduce noise in the text and focus on the more important words.\n",
    "3. Tokenise the text: Tokenising the text involves splitting the text into smaller units called tokens. Tokenising the text is important for many natural language processing tasks because it allows us to work with individual words rather than the entire text.\n",
    "4. Remove stop words: Stop words are common words that do not have a significant meaning and are usually removed from text before processing. Removing stop words helps to reduce the size of the vocabulary and focus on the more important words.\n",
    "5. Stem the words: Stemming involves reducing words to their base form. Stemming helps to reduce the size of the vocabulary and can also help to improve the performance of some natural language processing tasks.\n",
    "6. Create a Bag of Words to represent the text as a bag of its words, disregarding grammar and word order, but keeping track of the frequency of each word. This representation is used for simple tasks such as document classification and clustering and it is our choice for this assignment. Word Embedding represents each word as a dense vector in a continuous vector space, where semantically similar words are close to each other. This representation is used for more advanced tasks such as language translation and text generation. If the goal of the assignment is to perform a basic text classification task, using a Bag of Words representation may be sufficient and more appropriate than implementing a Word Embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c993cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all text in the 'text' column\n",
    "df['text']=df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4afc8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_1500\\4279493581.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "# remove all punctuation from the 'text' column\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ccf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the text in the 'text' column by splitting on whitespace\n",
    "df['text'] = df['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b766c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the 'text' column\n",
    "stop_words = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d184b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Porter stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d148863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words in the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c2d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'text' column to a numpy array\n",
    "x=df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39d51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the 'True/Fake' column to a numeric label\n",
    "df['label']=df['True/Fake'].map({\"true\":1,\"fake\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5449ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'label' column to a numpy array\n",
    "y=df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43e3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the tokenized words in the 'text' column back into a single string\n",
    "# only necessary because we are not able to vectorise a list of strings\n",
    "text = [' '.join(t) for t in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "418ce10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CountVectorizer\n",
    "vectoriser = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72f08041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words representation for the 'text' column\n",
    "X = vectoriser.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2fecf",
   "metadata": {},
   "source": [
    "## 6. Baseline performance\n",
    "##### Describe and justify the baseline against which you are going to compare the performance of your chosen approach. This can be an already published baseline (e.g. cited in the literature) or the results of a basic algorithm that you implement yourself. The baseline should represent a meaningful benchmark for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c9e82",
   "metadata": {},
   "source": [
    "The benchmark we will be using is 0.99 accuracy as stated in one of the implementations of a Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0ac9a",
   "metadata": {},
   "source": [
    "## 7. Classification approach\n",
    "##### Identify any features and labels which will be used in your classifier and justify why they were selected. Build a classifier using an appropriate Python library. Describe your chosen approach, e.g. random forest, support vector machine, Naïve Bayes, logistic regression, etc. and the rationale for selecting it. Run and evaluate your text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519f390",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier is a probabilistic machine learning model that is commonly used for classification tasks. It works by using Bayes' theorem to calculate the probability that an input belongs to each class, given the input's features. The class with the highest probability is then chosen as the predicted class.\n",
    "\n",
    "In the context of the given dataset, the Naive Bayes classifier is being used to classify text data into two categories: true or fake. To do this, the classifier first converts the text data into a numerical representation, using the technique of bag-of-words. It then estimates the probability of each class, given the input features (i.e. the bag-of-words representation of the text data). The class with the highest probability is then chosen as the predicted label.\n",
    "\n",
    "For example, suppose the Naive Bayes classifier is given an input text that consists of the following words: \"republican\", \"budget\", \"fiscal\", \"tax\", \"cut\". Based on the training data, the classifier might estimate the following probabilities for each class:\n",
    "\n",
    "    P(true | \"republican\", \"budget\", \"fiscal\", \"tax\", \"cut\") = 0.7\n",
    "    P(fake | \"republican\", \"budget\", \"fiscal\", \"tax\", \"cut\") = 0.3\n",
    "\n",
    "In this case, the classifier would predict the label \"true\" for this input, as the probability of the \"true\" class is higher.\n",
    "\n",
    "Overall, the Naive Bayes classifier is a simple yet effective tool for text classification tasks, as it is able to handle high-dimensional data and make predictions quickly.\n",
    "\n",
    "There are several reasons why a Naive Bayes classifier might be preferred over other types of classifiers, such as logistic regression, random forests, or support vector machines for this assignment.\n",
    "\n",
    "1. Simplicity: Naive Bayes classifiers are simple and easy to implement, as they make strong assumptions about the independence of the features. This makes them particularly useful for text classification tasks, where the data may have high dimensionality and the number of training examples may be limited.\n",
    "\n",
    "2. Scalability: Naive Bayes classifiers are highly scalable, as the training time and prediction time are both linear in the number of features and the number of training examples. This makes them suitable for large-scale classification tasks.\n",
    "\n",
    "4. Fast training and prediction times: Naive Bayes classifiers have fast training and prediction times, as they use a simple and efficient algorithm. This makes them useful for real-time classification tasks, where fast response times are important.\n",
    "\n",
    "In summary, the choice of a Naive Bayes classifier for text classification is motivated by its simplicity, scalability, and fast training and prediction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "762595be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,df['label'],random_state=42,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c054b19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c3dbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3693bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7bfc358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96      4650\n",
      "           1       0.94      0.97      0.95      4330\n",
      "\n",
      "    accuracy                           0.95      8980\n",
      "   macro avg       0.95      0.96      0.95      8980\n",
      "weighted avg       0.96      0.95      0.95      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab08ef",
   "metadata": {},
   "source": [
    "## 8. Coding style\n",
    "##### Your code is expected to meet certain standards as described by accepted coding conventions. This includes code indentation, avoiding unnamed numerical constants and undue use of string literals, assigning meaningful names to variables and subroutines, etc. The code is expected to be fully commented, including variables, sub-routines and calls to library methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecec32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f55c5",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "## 9. Evaluation\n",
    "##### Evaluate your classifier on the data set. Use the metrics you identified above to quantitatively evaluate the performance of your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fa0c8",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of our Naive Bayes classifier, we split our dataset into a training set and a test set, with the test set comprising 20% of the original dataset. We then trained the classifier on the training set and used it to predict the labels for the instances in the test set. Finally, we used the classification report to quantify the performance of the classifier, specifically in terms of precision, recall, and f1-score. The results show that the classifier performs well, with an f1-score of 95%. This is a good indication of the classifier's ability to accurately predict the labels for the test set instances, although there is still room for improvement with a benchmark of 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53313c",
   "metadata": {},
   "source": [
    "## 10. Summary and conclusions\n",
    "##### Provide a reflective evaluation of the project in light of your results. Describe its contributions to the problem area, and discuss the extent to which your solution is transferable to other domain-specific areas. Discuss the extent to which your approach can be replicated by others, e.g. using different programming languages, development environments, libraries and algorithms. Review the potential benefits and drawbacks of alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906bc53",
   "metadata": {},
   "source": [
    "The project aimed to build a text classifier to distinguish between true and fake news. We used a Naive Bayes classifier and achieved a classification accuracy of 95%. While this result is slightly lower than the benchmark of 99%, it still represents a strong performance and demonstrates the effectiveness of the chosen approach.\n",
    "\n",
    "One of the main contributions of this project is its use of natural language processing techniques to pre-process the text data. By lowercasing, removing punctuation, and stemming the words in the articles, we were able to filter out noise and focus on the relevant content. This is a crucial step in any text classification task, as it allows the classifier to better learn the patterns and features that distinguish true from fake news.\n",
    "\n",
    "In terms of transferability, the approach used in this project could be applied to other domain-specific areas where classification of text data is needed. For example, it could potentially be used to classify customer reviews as positive or negative, or to distinguish between spam and legitimate emails. However, it is important to note that the performance of the classifier may vary depending on the specific characteristics of the data and the task at hand.\n",
    "\n",
    "In terms of replicability, the approach used in this project could be easily replicated by others using different programming languages and libraries. For example, the same techniques could be implemented using Python's scikit-learn library, or using R's caret package. However, it is important to carefully consider the choice of algorithms and techniques, as different approaches may lead to different results.\n",
    "\n",
    "Overall, while the Naive Bayes classifier used in this project demonstrated strong performance in classifying true and fake news, it is important to also consider alternative approaches. For example, other machine learning algorithms such as logistic regression or support vector machines may also be effective in this task. It is important to carefully evaluate the trade-offs between different approaches and choose the one that best meets the needs of the specific application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
